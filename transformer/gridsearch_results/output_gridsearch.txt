Sender: LSF System <lsfadmin@eu-a6-007-21>
Subject: Job 227548143: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:03 2022
Job was executed on host(s) <2*eu-a6-007-21>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 10:58:38 2022
Results reported at Fri Aug  5 10:58:38 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   11.00 sec.
    Max Memory :                                 963 MB
    Average Memory :                             99.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7229.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   34 sec.
    Turnaround time :                            35 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 48, in get_module_cache
    return _get_module_cache(config.compiledir, init_args=init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1595, in get_module_cache
    _module_cache = ModuleCache(dirname, **init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 683, in __init__
    self.refresh()
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 773, in refresh
    files = os.listdir(root)
FileNotFoundError: [Errno 2] No such file or directory: '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/tmppdti216c'
Sender: LSF System <lsfadmin@eu-a6-007-02>
Subject: Job 227548102: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:59 2022
Job was executed on host(s) <2*eu-a6-007-02>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 10:58:38 2022
Results reported at Fri Aug  5 10:58:38 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   11.59 sec.
    Max Memory :                                 1054 MB
    Average Memory :                             155.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7138.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   36 sec.
    Turnaround time :                            39 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 48, in get_module_cache
    return _get_module_cache(config.compiledir, init_args=init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1595, in get_module_cache
    _module_cache = ModuleCache(dirname, **init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 683, in __init__
    self.refresh()
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 773, in refresh
    files = os.listdir(root)
FileNotFoundError: [Errno 2] No such file or directory: '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/tmpvwj8_064'
Sender: LSF System <lsfadmin@eu-a6-001-19>
Subject: Job 227548076: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:53 2022
Job was executed on host(s) <2*eu-a6-001-19>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 10:58:39 2022
Results reported at Fri Aug  5 10:58:39 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   11.54 sec.
    Max Memory :                                 1056 MB
    Average Memory :                             144.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7136.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   20 sec.
    Turnaround time :                            46 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 48, in get_module_cache
    return _get_module_cache(config.compiledir, init_args=init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1595, in get_module_cache
    _module_cache = ModuleCache(dirname, **init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 683, in __init__
    self.refresh()
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 773, in refresh
    files = os.listdir(root)
FileNotFoundError: [Errno 2] No such file or directory: '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/tmpz97n652l'
Sender: LSF System <lsfadmin@eu-a6-001-05>
Subject: Job 227548207: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:13 2022
Job was executed on host(s) <2*eu-a6-001-05>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 10:58:39 2022
Results reported at Fri Aug  5 10:58:39 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   11.92 sec.
    Max Memory :                                 1042 MB
    Average Memory :                             138.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7150.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   21 sec.
    Turnaround time :                            26 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 48, in get_module_cache
    return _get_module_cache(config.compiledir, init_args=init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1595, in get_module_cache
    _module_cache = ModuleCache(dirname, **init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 683, in __init__
    self.refresh()
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 773, in refresh
    files = os.listdir(root)
OSError: [Errno 116] Stale file handle: '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/tmpasv1xt09'
Sender: LSF System <lsfadmin@eu-c7-119-05>
Subject: Job 227547877: <python -m transformer.submit_gridsearch> in cluster <euler> Done

Job <python -m transformer.submit_gridsearch> was submitted from host <eu-login-47> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:02 2022
Job was executed on host(s) <eu-c7-119-05>, in queue <normal.4h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:57:23 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:57:23 2022
Terminated at Fri Aug  5 10:59:24 2022
Results reported at Fri Aug  5 10:59:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.submit_gridsearch
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   68.49 sec.
    Max Memory :                                 15 MB
    Average Memory :                             13.00 MB
    Total Requested Memory :                     2048.00 MB
    Delta Memory :                               2033.00 MB
    Max Swap :                                   -
    Max Processes :                              10
    Max Threads :                                11
    Run time :                                   140 sec.
    Turnaround time :                            142 sec.

The output (if any) follows:

Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548076> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548078> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548088> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548091> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548093> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548097> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548099> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548102> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548108> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548110> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548114> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548117> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548119> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548123> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548125> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548129> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548133> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548135> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548139> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548143> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548146> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548151> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548155> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548157> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548161> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548164> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548166> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548174> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548176> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548179> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548181> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548184> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548187> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548196> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548199> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548201> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548205> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548207> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548209> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548212> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548215> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548217> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548220> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548223> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548225> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548228> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548231> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548234> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548238> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548240> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548243> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548247> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548249> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548252> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548254> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548257> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548262> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548265> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548267> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548270> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548273> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548277> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548279> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548282> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548286> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548288> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548291> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548294> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548296> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548298> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548299> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548301> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548305> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548308> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548311> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548313> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548317> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548320> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548322> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548325> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548328> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548331> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548335> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548337> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548340> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548343> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548345> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548349> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548351> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548354> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548357> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548359> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548362> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548363> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548366> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548370> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548374> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548379> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548381> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548384> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548387> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548390> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548393> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548395> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548398> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548400> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548401> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548403> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548406> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548409> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548411> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548413> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548417> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548420> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548423> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548426> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548429> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548432> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548434> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548436> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548443> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548446> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548449> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548451> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548454> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548456> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548460> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548463> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548465> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548468> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548470> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548473> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548477> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548479> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548482> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548485> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548487> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548490> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548492> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548495> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548498> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548502> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548505> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548507> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548510> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548513> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548515> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548519> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548521> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548530> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548532> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548536> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548540> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548542> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548545> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548548> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548550> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548553> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548556> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548558> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548562> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548565> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548567> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548571> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548574> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548577> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548580> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548582> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548585> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548587> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548588> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548591> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548594> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548597> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548600> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548602> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548605> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548608> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548611> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548614> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548617> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548620> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548628> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548630> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548634> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548637> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548639> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548642> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548645> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548646> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548648> is submitted to queue <normal.120h>.
Warning: jobs longer than 5 days may be terminated at any time for operational reasons.
Generic job.
Job <227548651> is submitted to queue <normal.120h>.
Sender: LSF System <lsfadmin@eu-a2p-153>
Subject: Job 227548387: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:41 2022
Job was executed on host(s) <2*eu-a2p-153>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:12 2022
Results reported at Fri Aug  5 14:14:12 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.10 sec.
    Max Memory :                                 859 MB
    Average Memory :                             754.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7333.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   158 sec.
    Turnaround time :                            11731 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548417: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:46 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:13 2022
Results reported at Fri Aug  5 14:14:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   13.91 sec.
    Max Memory :                                 980 MB
    Average Memory :                             853.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7212.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   143 sec.
    Turnaround time :                            11727 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-169>
Subject: Job 227548340: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:35 2022
Job was executed on host(s) <2*eu-a2p-169>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:13 2022
Results reported at Fri Aug  5 14:14:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.77 sec.
    Max Memory :                                 853 MB
    Average Memory :                             747.57 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7339.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   170 sec.
    Turnaround time :                            11738 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-149>
Subject: Job 227548562: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:08 2022
Job was executed on host(s) <2*eu-a2p-149>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:13 2022
Results reported at Fri Aug  5 14:14:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.09 sec.
    Max Memory :                                 897 MB
    Average Memory :                             790.71 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7295.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   144 sec.
    Turnaround time :                            11705 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-149>
Subject: Job 227548556: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:07 2022
Job was executed on host(s) <2*eu-a2p-149>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.84 sec.
    Max Memory :                                 821 MB
    Average Memory :                             592.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7371.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   145 sec.
    Turnaround time :                            11707 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-191>
Subject: Job 227548311: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:32 2022
Job was executed on host(s) <2*eu-a2p-191>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   15.46 sec.
    Max Memory :                                 850 MB
    Average Memory :                             743.71 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7342.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   170 sec.
    Turnaround time :                            11742 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548460: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:53 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.11 sec.
    Max Memory :                                 934 MB
    Average Memory :                             813.14 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7258.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   157 sec.
    Turnaround time :                            11721 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548413: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:46 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.17 sec.
    Max Memory :                                 843 MB
    Average Memory :                             512.57 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7349.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   143 sec.
    Turnaround time :                            11728 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548409: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:45 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.63 sec.
    Max Memory :                                 858 MB
    Average Memory :                             528.43 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7334.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   143 sec.
    Turnaround time :                            11729 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548456: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:53 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.60 sec.
    Max Memory :                                 851 MB
    Average Memory :                             734.29 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7341.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   157 sec.
    Turnaround time :                            11721 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548451: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:52 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.59 sec.
    Max Memory :                                 854 MB
    Average Memory :                             736.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7338.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   157 sec.
    Turnaround time :                            11722 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-201>
Subject: Job 227548487: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:56 2022
Job was executed on host(s) <2*eu-a2p-201>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:14 2022
Results reported at Fri Aug  5 14:14:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.43 sec.
    Max Memory :                                 837 MB
    Average Memory :                             730.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7355.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   169 sec.
    Turnaround time :                            11718 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-158>
Subject: Job 227548240: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:18 2022
Job was executed on host(s) <2*eu-a2p-158>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:15 2022
Results reported at Fri Aug  5 14:14:15 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   13.40 sec.
    Max Memory :                                 1267 MB
    Average Memory :                             1120.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6925.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   149 sec.
    Turnaround time :                            11757 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-268>
Subject: Job 227548374: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:39 2022
Job was executed on host(s) <2*eu-a2p-268>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:15 2022
Results reported at Fri Aug  5 14:14:15 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.46 sec.
    Max Memory :                                 902 MB
    Average Memory :                             745.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7290.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   141 sec.
    Turnaround time :                            11736 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-172>
Subject: Job 227548429: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:48 2022
Job was executed on host(s) <2*eu-a2p-172>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:15 2022
Results reported at Fri Aug  5 14:14:15 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   15.20 sec.
    Max Memory :                                 834 MB
    Average Memory :                             726.71 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7358.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   172 sec.
    Turnaround time :                            11727 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-138>
Subject: Job 227548286: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:29 2022
Job was executed on host(s) <2*eu-a2p-138>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:16 2022
Results reported at Fri Aug  5 14:14:16 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.00 sec.
    Max Memory :                                 924 MB
    Average Memory :                             803.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7268.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   161 sec.
    Turnaround time :                            11747 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-172>
Subject: Job 227548432: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:48 2022
Job was executed on host(s) <2*eu-a2p-172>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:16 2022
Results reported at Fri Aug  5 14:14:16 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.66 sec.
    Max Memory :                                 827 MB
    Average Memory :                             591.43 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7365.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   173 sec.
    Turnaround time :                            11728 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-239>
Subject: Job 227548550: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:07 2022
Job was executed on host(s) <2*eu-a2p-239>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:16 2022
Results reported at Fri Aug  5 14:14:16 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   13.98 sec.
    Max Memory :                                 889 MB
    Average Memory :                             777.71 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7303.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   171 sec.
    Turnaround time :                            11709 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-239>
Subject: Job 227548545: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:06 2022
Job was executed on host(s) <2*eu-a2p-239>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:16 2022
Results reported at Fri Aug  5 14:14:16 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   14.15 sec.
    Max Memory :                                 846 MB
    Average Memory :                             739.29 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7346.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   171 sec.
    Turnaround time :                            11710 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-153>
Subject: Job 227548384: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:40 2022
Job was executed on host(s) <2*eu-a2p-153>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:19 2022
Results reported at Fri Aug  5 14:14:19 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   15.46 sec.
    Max Memory :                                 957 MB
    Average Memory :                             795.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7235.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   164 sec.
    Turnaround time :                            11739 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1157, in module_from_key
    module = self._get_from_hash(module_hash, key)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1058, in _get_from_hash
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-158>
Subject: Job 227548645: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:21 2022
Job was executed on host(s) <2*eu-a2p-158>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:16 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:16 2022
Terminated at Fri Aug  5 14:14:31 2022
Results reported at Fri Aug  5 14:14:31 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   11.55 sec.
    Max Memory :                                 1242 MB
    Average Memory :                             308.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6950.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   14 sec.
    Turnaround time :                            11710 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 48, in get_module_cache
    return _get_module_cache(config.compiledir, init_args=init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1595, in get_module_cache
    _module_cache = ModuleCache(dirname, **init_args)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 683, in __init__
    self.refresh()
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 773, in refresh
    files = os.listdir(root)
FileNotFoundError: [Errno 2] No such file or directory: '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/tmp3lrb0g3a'
Sender: LSF System <lsfadmin@eu-a2p-195>
Subject: Job 227548468: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:54 2022
Job was executed on host(s) <2*eu-a2p-195>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:38 2022
Results reported at Fri Aug  5 14:14:38 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   15.43 sec.
    Max Memory :                                 885 MB
    Average Memory :                             744.29 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7307.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   194 sec.
    Turnaround time :                            11744 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1161, in module_from_key
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a2p-138>
Subject: Job 227548282: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Exited

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:28 2022
Job was executed on host(s) <2*eu-a2p-138>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Fri Aug  5 14:14:40 2022
Results reported at Fri Aug  5 14:14:40 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   15.39 sec.
    Max Memory :                                 982 MB
    Average Memory :                             823.29 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7210.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   185 sec.
    Turnaround time :                            11772 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
Traceback (most recent call last):
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization/transformer/run_gridsearch.py", line 46, in <module>
    trace = pickle.load(fp)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1165, in _constructor_Function
    f = maker.create(input_storage, trustme=True)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/function/types.py", line 1836, in create
    _fn, _i, _o = self.linker.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/basic.py", line 266, in make_thunk
    return self.make_all(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/vm.py", line 1131, in make_all
    node.op.make_thunk(node, storage_map, compute_map, [], impl=impl)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 634, in make_thunk
    return self.make_c_thunk(node, storage_map, compute_map, no_recycling)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/graph/op.py", line 600, in make_c_thunk
    outputs = cl.make_thunk(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1203, in make_thunk
    cthunk, module, in_storage, out_storage, error_storage = self.__compile__(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1138, in __compile__
    thunk, module = self.cthunk_factory(
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/basic.py", line 1634, in cthunk_factory
    module = get_module_cache().module_from_key(key=key, lnk=self)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1157, in module_from_key
    module = self._get_from_hash(module_hash, key)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/link/c/cmodule.py", line 1058, in _get_from_hash
    with lock_ctx():
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/theano/compile/compilelock.py", line 73, in lock_ctx
    fl.acquire(timeout=timeout)
  File "/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/filelock/_api.py", line 183, in acquire
    raise Timeout(self._lock_file)
filelock._error.Timeout: The file lock '/cluster/home/garcieri/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-glibc2.17-x86_64-3.8.10-64/.lock' could not be acquired.
Sender: LSF System <lsfadmin@eu-a6-007-16>
Subject: Job 227548110: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:00 2022
Job was executed on host(s) <2*eu-a6-007-16>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:02:23 2022
Results reported at Fri Aug  5 21:02:23 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   39419.60 sec.
    Max Memory :                                 2209 MB
    Average Memory :                             1881.09 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5983.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   36260 sec.
    Turnaround time :                            36263 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:09.920870: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-003-24>
Subject: Job 227548157: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:06 2022
Job was executed on host(s) <2*eu-a6-003-24>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:03:14 2022
Results reported at Fri Aug  5 21:03:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   39376.09 sec.
    Max Memory :                                 2073 MB
    Average Memory :                             1743.30 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6119.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                47
    Run time :                                   36297 sec.
    Turnaround time :                            36308 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:16.431984: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-003-24>
Subject: Job 227548161: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:06 2022
Job was executed on host(s) <2*eu-a6-003-24>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:14:43 2022
Results reported at Fri Aug  5 21:14:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   40134.08 sec.
    Max Memory :                                 2027 MB
    Average Memory :                             1670.42 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6165.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                47
    Run time :                                   36985 sec.
    Turnaround time :                            36997 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:16.955867: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-003-21>
Subject: Job 227548099: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:59 2022
Job was executed on host(s) <2*eu-a6-003-21>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:27:45 2022
Results reported at Fri Aug  5 21:27:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   41075.94 sec.
    Max Memory :                                 2241 MB
    Average Memory :                             1957.97 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5951.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   37782 sec.
    Turnaround time :                            37786 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:18.233463: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-007-22>
Subject: Job 227548133: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:02 2022
Job was executed on host(s) <2*eu-a6-007-22>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:29:33 2022
Results reported at Fri Aug  5 21:29:33 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   41153.79 sec.
    Max Memory :                                 2279 MB
    Average Memory :                             1936.07 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5913.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   37880 sec.
    Turnaround time :                            37891 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:17.544797: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-007-10>
Subject: Job 227548091: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:58 2022
Job was executed on host(s) <2*eu-a6-007-10>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:44:14 2022
Results reported at Fri Aug  5 21:44:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42071.82 sec.
    Max Memory :                                 2199 MB
    Average Memory :                             1516.36 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5993.00 MB
    Max Swap :                                   538 MB
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   38754 sec.
    Turnaround time :                            38776 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:37.975230: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-001-24>
Subject: Job 227548088: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:57 2022
Job was executed on host(s) <2*eu-a6-001-24>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:45:32 2022
Results reported at Fri Aug  5 21:45:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42247.09 sec.
    Max Memory :                                 2240 MB
    Average Memory :                             1891.90 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5952.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   38849 sec.
    Turnaround time :                            38855 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:18.203592: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-001-02>
Subject: Job 227548123: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:01 2022
Job was executed on host(s) <2*eu-a6-001-02>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:45:38 2022
Results reported at Fri Aug  5 21:45:38 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42279.15 sec.
    Max Memory :                                 2003 MB
    Average Memory :                             1511.15 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6189.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   38845 sec.
    Turnaround time :                            38857 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:15.361611: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-001-02>
Subject: Job 227548129: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:02 2022
Job was executed on host(s) <2*eu-a6-001-02>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:46:39 2022
Results reported at Fri Aug  5 21:46:39 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42314.85 sec.
    Max Memory :                                 1834 MB
    Average Memory :                             1459.38 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6358.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   38906 sec.
    Turnaround time :                            38917 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:17.650375: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-001-02>
Subject: Job 227548125: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:02 2022
Job was executed on host(s) <2*eu-a6-001-02>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:47:43 2022
Results reported at Fri Aug  5 21:47:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42372.29 sec.
    Max Memory :                                 1982 MB
    Average Memory :                             1558.99 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6210.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   38970 sec.
    Turnaround time :                            38981 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:19.118725: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-001-04>
Subject: Job 227548135: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:03 2022
Job was executed on host(s) <2*eu-a6-001-04>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:48:00 2022
Results reported at Fri Aug  5 21:48:00 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42334.33 sec.
    Max Memory :                                 2152 MB
    Average Memory :                             1788.26 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6040.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   38982 sec.
    Turnaround time :                            38997 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:18.122851: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-001-04>
Subject: Job 227548139: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:03 2022
Job was executed on host(s) <2*eu-a6-001-04>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:51:24 2022
Results reported at Fri Aug  5 21:51:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42589.56 sec.
    Max Memory :                                 1968 MB
    Average Memory :                             1624.60 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6224.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   39187 sec.
    Turnaround time :                            39201 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:22.558022: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-001-10>
Subject: Job 227548114: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:00 2022
Job was executed on host(s) <2*eu-a6-001-10>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:55:46 2022
Results reported at Fri Aug  5 21:55:46 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   42946.16 sec.
    Max Memory :                                 2244 MB
    Average Memory :                             2058.16 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5948.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   39444 sec.
    Turnaround time :                            39466 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:22.965175: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548179: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:10 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:58:07 2022
Results reported at Fri Aug  5 21:58:07 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43122.62 sec.
    Max Memory :                                 1863 MB
    Average Memory :                             1443.34 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6329.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   39604 sec.
    Turnaround time :                            39597 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:21.607068: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548184: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:10 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 21:59:10 2022
Results reported at Fri Aug  5 21:59:10 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43115.05 sec.
    Max Memory :                                 2057 MB
    Average Memory :                             1671.45 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6135.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   39667 sec.
    Turnaround time :                            39660 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:15.100273: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548181: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:10 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:00:51 2022
Results reported at Fri Aug  5 22:00:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43194.96 sec.
    Max Memory :                                 1929 MB
    Average Memory :                             1471.45 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6263.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   39768 sec.
    Turnaround time :                            39761 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:21.337550: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-001-16>
Subject: Job 227548097: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:58 2022
Job was executed on host(s) <2*eu-a6-001-16>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:04:43 2022
Results reported at Fri Aug  5 22:04:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43453.89 sec.
    Max Memory :                                 2238 MB
    Average Memory :                             1995.96 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5954.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   39991 sec.
    Turnaround time :                            40005 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:20.557262: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-007-04>
Subject: Job 227548119: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:01 2022
Job was executed on host(s) <2*eu-a6-007-04>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:10:49 2022
Results reported at Fri Aug  5 22:10:49 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43997.51 sec.
    Max Memory :                                 1870 MB
    Average Memory :                             1479.44 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6322.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   40366 sec.
    Turnaround time :                            40368 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:22.082961: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-007-04>
Subject: Job 227548117: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:01 2022
Job was executed on host(s) <2*eu-a6-007-04>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:11:31 2022
Results reported at Fri Aug  5 22:11:31 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   43960.55 sec.
    Max Memory :                                 2167 MB
    Average Memory :                             1809.13 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6025.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   40407 sec.
    Turnaround time :                            40410 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:19.464397: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-001-06>
Subject: Job 227548174: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:09 2022
Job was executed on host(s) <2*eu-a6-001-06>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:13:23 2022
Results reported at Fri Aug  5 22:13:23 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44045.72 sec.
    Max Memory :                                 2128 MB
    Average Memory :                             1786.20 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6064.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   40528 sec.
    Turnaround time :                            40514 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:19.033317: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-001-06>
Subject: Job 227548164: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:06 2022
Job was executed on host(s) <2*eu-a6-001-06>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:20:09 2022
Results reported at Fri Aug  5 22:20:09 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44471.16 sec.
    Max Memory :                                 1984 MB
    Average Memory :                             1591.87 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6208.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   40934 sec.
    Turnaround time :                            40923 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:21.280364: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-007-16>
Subject: Job 227548108: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:00 2022
Job was executed on host(s) <2*eu-a6-007-16>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:20:55 2022
Results reported at Fri Aug  5 22:20:55 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44670.46 sec.
    Max Memory :                                 1831 MB
    Average Memory :                             1444.87 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6361.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   40971 sec.
    Turnaround time :                            40975 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:23.844058: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-001-06>
Subject: Job 227548166: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:07 2022
Job was executed on host(s) <2*eu-a6-001-06>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 22:22:47 2022
Results reported at Fri Aug  5 22:22:47 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   44644.35 sec.
    Max Memory :                                 1981 MB
    Average Memory :                             1601.63 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6211.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   41092 sec.
    Turnaround time :                            41080 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:20.599163: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-007-21>
Subject: Job 227548151: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:05 2022
Job was executed on host(s) <2*eu-a6-007-21>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 23:01:00 2022
Results reported at Fri Aug  5 23:01:00 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   47156.49 sec.
    Max Memory :                                 1876 MB
    Average Memory :                             1467.75 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6316.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   43376 sec.
    Turnaround time :                            43375 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:26.067193: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-007-21>
Subject: Job 227548155: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:05 2022
Job was executed on host(s) <2*eu-a6-007-21>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:23 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:23 2022
Terminated at Fri Aug  5 23:03:36 2022
Results reported at Fri Aug  5 23:03:36 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   47373.94 sec.
    Max Memory :                                 2025 MB
    Average Memory :                             1689.46 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6167.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   43531 sec.
    Turnaround time :                            43531 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:20.868404: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-004-20>
Subject: Job 227548093: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:58 2022
Job was executed on host(s) <2*eu-a6-004-20>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 23:04:09 2022
Results reported at Fri Aug  5 23:04:09 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   47526.31 sec.
    Max Memory :                                 2235 MB
    Average Memory :                             1914.65 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5957.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   43574 sec.
    Turnaround time :                            43571 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:34.158454: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a6-007-21>
Subject: Job 227548146: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:05 2022
Job was executed on host(s) <2*eu-a6-007-21>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 23:07:17 2022
Results reported at Fri Aug  5 23:07:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   47631.91 sec.
    Max Memory :                                 1869 MB
    Average Memory :                             1520.29 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6323.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   43752 sec.
    Turnaround time :                            43752 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:23.545479: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548176: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:09 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Fri Aug  5 23:34:15 2022
Results reported at Fri Aug  5 23:34:15 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   49430.54 sec.
    Max Memory :                                 1859 MB
    Average Memory :                             1483.37 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6333.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   45371 sec.
    Turnaround time :                            45366 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:32.260586: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-001-20>
Subject: Job 227548078: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:57:56 2022
Job was executed on host(s) <2*eu-a6-001-20>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Sat Aug  6 01:35:09 2022
Results reported at Sat Aug  6 01:35:09 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   56498.65 sec.
    Max Memory :                                 2243 MB
    Average Memory :                             1886.75 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5949.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   52627 sec.
    Turnaround time :                            52633 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:17.214478: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-268>
Subject: Job 227548379: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:40 2022
Job was executed on host(s) <2*eu-a2p-268>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 03:38:51 2022
Results reported at Sat Aug  6 03:38:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   48267.61 sec.
    Max Memory :                                 1767 MB
    Average Memory :                             1483.18 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6425.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   48416 sec.
    Turnaround time :                            60011 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:45.044550: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-164>
Subject: Job 227548403: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:45 2022
Job was executed on host(s) <2*eu-a2p-164>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 03:43:11 2022
Results reported at Sat Aug  6 03:43:11 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   48548.11 sec.
    Max Memory :                                 1803 MB
    Average Memory :                             1582.56 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6389.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   48681 sec.
    Turnaround time :                            60266 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:47.332355: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-153>
Subject: Job 227548393: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:41 2022
Job was executed on host(s) <2*eu-a2p-153>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:06:29 2022
Results reported at Sat Aug  6 04:06:29 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   49892.48 sec.
    Max Memory :                                 2001 MB
    Average Memory :                             1709.01 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6191.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   50095 sec.
    Turnaround time :                            61668 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:53.485621: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-153>
Subject: Job 227548381: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:40 2022
Job was executed on host(s) <2*eu-a2p-153>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:13:26 2022
Results reported at Sat Aug  6 04:13:26 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   50320.90 sec.
    Max Memory :                                 1777 MB
    Average Memory :                             1442.16 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6415.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   50512 sec.
    Turnaround time :                            62086 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:55.625645: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-164>
Subject: Job 227548395: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:43 2022
Job was executed on host(s) <2*eu-a2p-164>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:14:27 2022
Results reported at Sat Aug  6 04:14:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   50398.55 sec.
    Max Memory :                                 2054 MB
    Average Memory :                             1817.94 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6138.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   50556 sec.
    Turnaround time :                            62144 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:47.212351: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-177>
Subject: Job 227548449: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:51 2022
Job was executed on host(s) <2*eu-a2p-177>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:20:43 2022
Results reported at Sat Aug  6 04:20:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   50776.59 sec.
    Max Memory :                                 1851 MB
    Average Memory :                             1491.31 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6341.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   50960 sec.
    Turnaround time :                            62512 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:01.581538: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548411: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:46 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:30:51 2022
Results reported at Sat Aug  6 04:30:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   51440.08 sec.
    Max Memory :                                 1968 MB
    Average Memory :                             1646.57 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6224.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   51540 sec.
    Turnaround time :                            63125 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:41.971959: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-177>
Subject: Job 227548434: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:48 2022
Job was executed on host(s) <2*eu-a2p-177>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:34:25 2022
Results reported at Sat Aug  6 04:34:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   51571.93 sec.
    Max Memory :                                 1911 MB
    Average Memory :                             1762.99 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6281.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   51781 sec.
    Turnaround time :                            63337 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:59.772309: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548406: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:45 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:37:52 2022
Results reported at Sat Aug  6 04:37:52 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   51795.36 sec.
    Max Memory :                                 1788 MB
    Average Memory :                             1468.45 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6404.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   51962 sec.
    Turnaround time :                            63547 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:55.321962: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-164>
Subject: Job 227548400: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:44 2022
Job was executed on host(s) <2*eu-a2p-164>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:40:02 2022
Results reported at Sat Aug  6 04:40:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   52003.23 sec.
    Max Memory :                                 1806 MB
    Average Memory :                             1468.54 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6386.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   52092 sec.
    Turnaround time :                            63678 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:58.562348: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548463: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:53 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:44:00 2022
Results reported at Sat Aug  6 04:44:00 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   52245.14 sec.
    Max Memory :                                 2021 MB
    Average Memory :                             1665.39 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6171.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   52342 sec.
    Turnaround time :                            63907 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:58.206976: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-177>
Subject: Job 227548443: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:51 2022
Job was executed on host(s) <2*eu-a2p-177>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 04:57:36 2022
Results reported at Sat Aug  6 04:57:36 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   53032.26 sec.
    Max Memory :                                 1826 MB
    Average Memory :                             1644.03 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6366.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   53172 sec.
    Turnaround time :                            64725 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:02.232298: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-172>
Subject: Job 227548420: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:47 2022
Job was executed on host(s) <2*eu-a2p-172>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 05:01:33 2022
Results reported at Sat Aug  6 05:01:33 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   53204.07 sec.
    Max Memory :                                 2117 MB
    Average Memory :                             1757.05 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6075.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   53409 sec.
    Turnaround time :                            64966 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:55.907698: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-172>
Subject: Job 227548426: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:47 2022
Job was executed on host(s) <2*eu-a2p-172>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 05:06:47 2022
Results reported at Sat Aug  6 05:06:47 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   53586.08 sec.
    Max Memory :                                 1824 MB
    Average Memory :                             1503.75 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6368.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   53723 sec.
    Turnaround time :                            65280 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:02.457704: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-150>
Subject: Job 227548243: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:19 2022
Job was executed on host(s) <2*eu-a2p-150>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 07:36:03 2022
Results reported at Sat Aug  6 07:36:03 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   62496.61 sec.
    Max Memory :                                 2702 MB
    Average Memory :                             1932.74 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5490.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   62649 sec.
    Turnaround time :                            74264 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:25.677916: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-256>
Subject: Job 227548254: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:20 2022
Job was executed on host(s) <2*eu-a2p-256>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 07:43:53 2022
Results reported at Sat Aug  6 07:43:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   62954.89 sec.
    Max Memory :                                 2810 MB
    Average Memory :                             2041.29 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5382.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   63119 sec.
    Turnaround time :                            74733 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:03.546782: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-174>
Subject: Job 227548279: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:28 2022
Job was executed on host(s) <2*eu-a2p-174>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 07:50:02 2022
Results reported at Sat Aug  6 07:50:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   63336.71 sec.
    Max Memory :                                 2756 MB
    Average Memory :                             1987.18 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5436.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   63519 sec.
    Turnaround time :                            75094 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:29.975109: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-256>
Subject: Job 227548262: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:23 2022
Job was executed on host(s) <2*eu-a2p-256>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 08:18:04 2022
Results reported at Sat Aug  6 08:18:04 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   65051.18 sec.
    Max Memory :                                 2737 MB
    Average Memory :                             1969.24 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5455.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   65170 sec.
    Turnaround time :                            76781 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:16.716637: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-150>
Subject: Job 227548249: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:19 2022
Job was executed on host(s) <2*eu-a2p-150>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 08:20:26 2022
Results reported at Sat Aug  6 08:20:26 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   65188.34 sec.
    Max Memory :                                 2831 MB
    Average Memory :                             2063.21 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5361.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   65311 sec.
    Turnaround time :                            76927 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:18.237897: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-174>
Subject: Job 227548267: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:23 2022
Job was executed on host(s) <2*eu-a2p-174>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 08:21:05 2022
Results reported at Sat Aug  6 08:21:05 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   65172.48 sec.
    Max Memory :                                 2875 MB
    Average Memory :                             2105.85 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5317.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   65381 sec.
    Turnaround time :                            76962 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:21.750392: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-174>
Subject: Job 227548273: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:27 2022
Job was executed on host(s) <2*eu-a2p-174>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 08:26:51 2022
Results reported at Sat Aug  6 08:26:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   65556.08 sec.
    Max Memory :                                 2678 MB
    Average Memory :                             1908.20 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5514.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   65727 sec.
    Turnaround time :                            77304 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:29.090392: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-164>
Subject: Job 227548401: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:44 2022
Job was executed on host(s) <2*eu-a2p-164>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 08:39:59 2022
Results reported at Sat Aug  6 08:39:59 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   77295.98 sec.
    Max Memory :                                 1789 MB
    Average Memory :                             1502.97 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6403.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   66488 sec.
    Turnaround time :                            78075 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:55.299002: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-164>
Subject: Job 227548398: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:43 2022
Job was executed on host(s) <2*eu-a2p-164>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 08:56:53 2022
Results reported at Sat Aug  6 08:56:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   78478.18 sec.
    Max Memory :                                 1780 MB
    Average Memory :                             1603.02 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6412.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   67503 sec.
    Turnaround time :                            79090 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:00.338078: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-153>
Subject: Job 227548390: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:41 2022
Job was executed on host(s) <2*eu-a2p-153>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 09:07:57 2022
Results reported at Sat Aug  6 09:07:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   79216.91 sec.
    Max Memory :                                 1813 MB
    Average Memory :                             1488.04 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6379.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   68182 sec.
    Turnaround time :                            79756 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:56.272723: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.43032
********************************
Sender: LSF System <lsfadmin@eu-a2p-177>
Subject: Job 227548436: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:49 2022
Job was executed on host(s) <2*eu-a2p-177>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 09:33:13 2022
Results reported at Sat Aug  6 09:33:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   80743.17 sec.
    Max Memory :                                 1814 MB
    Average Memory :                             1499.37 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6378.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   69709 sec.
    Turnaround time :                            81264 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:58.710592: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-177>
Subject: Job 227548446: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:51 2022
Job was executed on host(s) <2*eu-a2p-177>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 09:39:33 2022
Results reported at Sat Aug  6 09:39:33 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   81492.19 sec.
    Max Memory :                                 1952 MB
    Average Memory :                             1595.21 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6240.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   70090 sec.
    Turnaround time :                            81642 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:36.328686: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-172>
Subject: Job 227548423: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:47 2022
Job was executed on host(s) <2*eu-a2p-172>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 10:07:17 2022
Results reported at Sat Aug  6 10:07:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   83289.48 sec.
    Max Memory :                                 1843 MB
    Average Memory :                             1498.83 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6349.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   71754 sec.
    Turnaround time :                            83310 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:00.004081: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548454: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:52 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 10:08:24 2022
Results reported at Sat Aug  6 10:08:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 2 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   83537.86 sec.
    Max Memory :                                 1833 MB
    Average Memory :                             1476.78 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               6359.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   71806 sec.
    Turnaround time :                            83372 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:00.209912: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.44034
********************************
Sender: LSF System <lsfadmin@eu-a6-001-05>
Subject: Job 227548220: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:16 2022
Job was executed on host(s) <2*eu-a6-001-05>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:51 2022
Terminated at Sat Aug  6 11:22:20 2022
Results reported at Sat Aug  6 11:22:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   94564.77 sec.
    Max Memory :                                 2766 MB
    Average Memory :                             2035.79 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5426.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   87816 sec.
    Turnaround time :                            87844 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:34.808443: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-001-23>
Subject: Job 227548225: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:17 2022
Job was executed on host(s) <2*eu-a6-001-23>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:51 2022
Terminated at Sat Aug  6 12:20:53 2022
Results reported at Sat Aug  6 12:20:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   98228.09 sec.
    Max Memory :                                 2951 MB
    Average Memory :                             2032.25 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5241.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   91323 sec.
    Turnaround time :                            91356 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:32.786929: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-001-15>
Subject: Job 227548223: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:16 2022
Job was executed on host(s) <2*eu-a6-001-15>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:51 2022
Terminated at Sat Aug  6 12:37:42 2022
Results reported at Sat Aug  6 12:37:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   99374.90 sec.
    Max Memory :                                 3234 MB
    Average Memory :                             2119.94 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               4958.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   92344 sec.
    Turnaround time :                            92366 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:37.195507: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-001-07>
Subject: Job 227548205: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:13 2022
Job was executed on host(s) <2*eu-a6-001-07>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Sat Aug  6 12:38:21 2022
Results reported at Sat Aug  6 12:38:21 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   99726.92 sec.
    Max Memory :                                 3060 MB
    Average Memory :                             2334.55 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5132.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                47
    Run time :                                   92420 sec.
    Turnaround time :                            92408 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:44.180310: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-007-21>
Subject: Job 227548215: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:15 2022
Job was executed on host(s) <2*eu-a6-007-21>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:39 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:39 2022
Terminated at Sat Aug  6 12:56:43 2022
Results reported at Sat Aug  6 12:56:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   100753.84 sec.
    Max Memory :                                 2581 MB
    Average Memory :                             1858.99 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5611.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   93483 sec.
    Turnaround time :                            93508 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:45.190127: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-256>
Subject: Job 227548265: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:23 2022
Job was executed on host(s) <2*eu-a2p-256>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 13:01:44 2022
Results reported at Sat Aug  6 13:01:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   87966.31 sec.
    Max Memory :                                 2677 MB
    Average Memory :                             1907.38 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5515.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   82190 sec.
    Turnaround time :                            93801 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:22.938831: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548196: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:12 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Sat Aug  6 13:14:28 2022
Results reported at Sat Aug  6 13:14:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   101987.48 sec.
    Max Memory :                                 2619 MB
    Average Memory :                             1859.08 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5573.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   94585 sec.
    Turnaround time :                            94576 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:42.297055: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548199: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:12 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Sat Aug  6 13:24:06 2022
Results reported at Sat Aug  6 13:24:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   102566.10 sec.
    Max Memory :                                 2693 MB
    Average Memory :                             1936.64 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5499.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   95163 sec.
    Turnaround time :                            95154 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:39.480189: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-150>
Subject: Job 227548252: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:20 2022
Job was executed on host(s) <2*eu-a2p-150>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 13:25:42 2022
Results reported at Sat Aug  6 13:25:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   89442.52 sec.
    Max Memory :                                 2851 MB
    Average Memory :                             2083.06 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5341.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   83628 sec.
    Turnaround time :                            95242 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:44.697284: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548187: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:11 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Sat Aug  6 13:25:44 2022
Results reported at Sat Aug  6 13:25:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   102693.21 sec.
    Max Memory :                                 2627 MB
    Average Memory :                             1866.27 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5565.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   95260 sec.
    Turnaround time :                            95253 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:39.512826: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-150>
Subject: Job 227548247: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:19 2022
Job was executed on host(s) <2*eu-a2p-150>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 13:27:01 2022
Results reported at Sat Aug  6 13:27:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   89428.46 sec.
    Max Memory :                                 2697 MB
    Average Memory :                             1928.63 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5495.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   83707 sec.
    Turnaround time :                            95322 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:11.276180: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a6-007-11>
Subject: Job 227548201: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:13 2022
Job was executed on host(s) <2*eu-a6-007-11>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:22 2022
Terminated at Sat Aug  6 13:27:11 2022
Results reported at Sat Aug  6 13:27:11 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   102898.83 sec.
    Max Memory :                                 2608 MB
    Average Memory :                             1847.38 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5584.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   95347 sec.
    Turnaround time :                            95338 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:42.992037: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-256>
Subject: Job 227548257: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:22 2022
Job was executed on host(s) <2*eu-a2p-256>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 13:35:57 2022
Results reported at Sat Aug  6 13:35:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   90014.81 sec.
    Max Memory :                                 2859 MB
    Average Memory :                             2090.66 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5333.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   84242 sec.
    Turnaround time :                            95855 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:17:57.595341: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-174>
Subject: Job 227548277: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:28 2022
Job was executed on host(s) <2*eu-a2p-174>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 13:42:04 2022
Results reported at Sat Aug  6 13:42:04 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   90417.42 sec.
    Max Memory :                                 2715 MB
    Average Memory :                             1946.89 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5477.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   84641 sec.
    Turnaround time :                            96216 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:29.911938: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-190>
Subject: Job 227548234: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:18 2022
Job was executed on host(s) <2*eu-a2p-190>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 14:11:02 2022
Results reported at Sat Aug  6 14:11:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   92304.02 sec.
    Max Memory :                                 2754 MB
    Average Memory :                             2054.36 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5438.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   86363 sec.
    Turnaround time :                            97964 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:21.764683: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-174>
Subject: Job 227548270: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:25 2022
Job was executed on host(s) <2*eu-a2p-174>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 14:16:54 2022
Results reported at Sat Aug  6 14:16:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   92571.76 sec.
    Max Memory :                                 2672 MB
    Average Memory :                             1903.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5520.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   86730 sec.
    Turnaround time :                            98309 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:26.147322: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-190>
Subject: Job 227548238: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:18 2022
Job was executed on host(s) <2*eu-a2p-190>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 14:20:01 2022
Results reported at Sat Aug  6 14:20:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   92862.65 sec.
    Max Memory :                                 3027 MB
    Average Memory :                             2259.88 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5165.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   86902 sec.
    Turnaround time :                            98503 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:25.056275: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a6-001-19>
Subject: Job 227548212: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:15 2022
Job was executed on host(s) <2*eu-a6-001-19>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:40 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:40 2022
Terminated at Sat Aug  6 14:29:06 2022
Results reported at Sat Aug  6 14:29:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   107137.43 sec.
    Max Memory :                                 2752 MB
    Average Memory :                             2049.72 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5440.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   99025 sec.
    Turnaround time :                            99051 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:49.851927: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-001-05>
Subject: Job 227548217: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:16 2022
Job was executed on host(s) <2*eu-a6-001-05>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:40 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:40 2022
Terminated at Sat Aug  6 14:41:34 2022
Results reported at Sat Aug  6 14:41:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   107749.94 sec.
    Max Memory :                                 2663 MB
    Average Memory :                             1909.28 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5529.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   99773 sec.
    Turnaround time :                            99798 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:53.806023: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-007-02>
Subject: Job 227548209: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:15 2022
Job was executed on host(s) <2*eu-a6-007-02>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 10:58:39 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 10:58:39 2022
Terminated at Sat Aug  6 14:56:18 2022
Results reported at Sat Aug  6 14:56:18 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   108315.03 sec.
    Max Memory :                                 2754 MB
    Average Memory :                             2014.31 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5438.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   100658 sec.
    Turnaround time :                            100683 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 11:02:51.998128: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-003-21>
Subject: Job 227548228: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:17 2022
Job was executed on host(s) <2*eu-a6-003-21>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 16:10:01 2022
Results reported at Sat Aug  6 16:10:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   101057.45 sec.
    Max Memory :                                 2759 MB
    Average Memory :                             2034.90 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5433.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   93519 sec.
    Turnaround time :                            105104 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:15:23.214849: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a6-007-07>
Subject: Job 227548231: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:17 2022
Job was executed on host(s) <2*eu-a6-007-07>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 17:00:24 2022
Results reported at Sat Aug  6 17:00:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   103992.63 sec.
    Max Memory :                                 3242 MB
    Average Memory :                             2483.20 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               4950.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   96525 sec.
    Turnaround time :                            108127 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:21.997056: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-216>
Subject: Job 227548505: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:00 2022
Job was executed on host(s) <2*eu-a2p-216>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 18:32:21 2022
Results reported at Sat Aug  6 18:32:21 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   101990.50 sec.
    Max Memory :                                 2505 MB
    Average Memory :                             1876.70 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5687.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   102041 sec.
    Turnaround time :                            113601 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:25.310273: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-201>
Subject: Job 227548490: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:57 2022
Job was executed on host(s) <2*eu-a2p-201>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 18:34:45 2022
Results reported at Sat Aug  6 18:34:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   102112.45 sec.
    Max Memory :                                 2503 MB
    Average Memory :                             1891.48 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5689.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   102199 sec.
    Turnaround time :                            113748 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:20.377311: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-201>
Subject: Job 227548479: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:55 2022
Job was executed on host(s) <2*eu-a2p-201>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 18:39:07 2022
Results reported at Sat Aug  6 18:39:07 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   102337.45 sec.
    Max Memory :                                 2657 MB
    Average Memory :                             2045.36 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5535.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   102462 sec.
    Turnaround time :                            114012 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:24.647298: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-239>
Subject: Job 227548553: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:07 2022
Job was executed on host(s) <2*eu-a2p-239>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 19:13:22 2022
Results reported at Sat Aug  6 19:13:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   104425.48 sec.
    Max Memory :                                 2704 MB
    Average Memory :                             2028.73 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5488.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   104516 sec.
    Turnaround time :                            116055 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:24.975817: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-193>
Subject: Job 227548519: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:02 2022
Job was executed on host(s) <2*eu-a2p-193>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 19:20:30 2022
Results reported at Sat Aug  6 19:20:30 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   104862.23 sec.
    Max Memory :                                 2881 MB
    Average Memory :                             2203.68 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5311.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   104945 sec.
    Turnaround time :                            116488 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:21.954182: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-216>
Subject: Job 227548492: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:57 2022
Job was executed on host(s) <2*eu-a2p-216>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 19:20:44 2022
Results reported at Sat Aug  6 19:20:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   104833.84 sec.
    Max Memory :                                 2506 MB
    Average Memory :                             1883.75 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5686.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   104944 sec.
    Turnaround time :                            116507 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:22.750267: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-196>
Subject: Job 227548540: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:04 2022
Job was executed on host(s) <2*eu-a2p-196>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 19:41:34 2022
Results reported at Sat Aug  6 19:41:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   106088.92 sec.
    Max Memory :                                 2579 MB
    Average Memory :                             1902.27 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5613.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                41
    Run time :                                   106179 sec.
    Turnaround time :                            117750 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:25.851695: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-193>
Subject: Job 227548507: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:00 2022
Job was executed on host(s) <2*eu-a2p-193>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 19:44:01 2022
Results reported at Sat Aug  6 19:44:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   106224.80 sec.
    Max Memory :                                 2526 MB
    Average Memory :                             1912.75 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5666.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   106356 sec.
    Turnaround time :                            117901 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:24.304173: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-201>
Subject: Job 227548485: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:56 2022
Job was executed on host(s) <2*eu-a2p-201>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 20:04:28 2022
Results reported at Sat Aug  6 20:04:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   107606.12 sec.
    Max Memory :                                 2530 MB
    Average Memory :                             1918.46 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5662.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   107582 sec.
    Turnaround time :                            119132 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:24.747307: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-239>
Subject: Job 227548542: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:06 2022
Job was executed on host(s) <2*eu-a2p-239>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 20:16:25 2022
Results reported at Sat Aug  6 20:16:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   108122.00 sec.
    Max Memory :                                 2583 MB
    Average Memory :                             1904.75 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5609.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   108299 sec.
    Turnaround time :                            119839 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:24.655829: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-216>
Subject: Job 227548498: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:59 2022
Job was executed on host(s) <2*eu-a2p-216>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 20:23:02 2022
Results reported at Sat Aug  6 20:23:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   108739.83 sec.
    Max Memory :                                 2669 MB
    Average Memory :                             1936.51 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5523.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   108683 sec.
    Turnaround time :                            120243 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:26.590277: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-196>
Subject: Job 227548521: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:02 2022
Job was executed on host(s) <2*eu-a2p-196>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 20:31:22 2022
Results reported at Sat Aug  6 20:31:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   109145.83 sec.
    Max Memory :                                 2574 MB
    Average Memory :                             1896.88 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5618.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   109168 sec.
    Turnaround time :                            120740 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:29.821686: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-239>
Subject: Job 227548548: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:06 2022
Job was executed on host(s) <2*eu-a2p-239>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 20:49:37 2022
Results reported at Sat Aug  6 20:49:37 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   110287.82 sec.
    Max Memory :                                 2762 MB
    Average Memory :                             2084.82 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5430.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   110291 sec.
    Turnaround time :                            121831 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:00.335833: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-195>
Subject: Job 227548465: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:54 2022
Job was executed on host(s) <2*eu-a2p-195>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 20:54:22 2022
Results reported at Sat Aug  6 20:54:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   110850.54 sec.
    Max Memory :                                 2601 MB
    Average Memory :                             1995.40 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5591.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   110577 sec.
    Turnaround time :                            122128 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:12.784051: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-193>
Subject: Job 227548513: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:01 2022
Job was executed on host(s) <2*eu-a2p-193>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 20:55:21 2022
Results reported at Sat Aug  6 20:55:21 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   110653.70 sec.
    Max Memory :                                 2589 MB
    Average Memory :                             1913.15 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5603.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   110636 sec.
    Turnaround time :                            122180 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:27.134174: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-196>
Subject: Job 227548532: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:04 2022
Job was executed on host(s) <2*eu-a2p-196>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 21:27:54 2022
Results reported at Sat Aug  6 21:27:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   112749.59 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2250.79 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5264.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   112559 sec.
    Turnaround time :                            124130 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:15.021688: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-195>
Subject: Job 227548470: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:54 2022
Job was executed on host(s) <2*eu-a2p-195>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 21:39:34 2022
Results reported at Sat Aug  6 21:39:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   113789.91 sec.
    Max Memory :                                 2561 MB
    Average Memory :                             1950.51 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5631.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   113290 sec.
    Turnaround time :                            124840 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:22.704050: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-195>
Subject: Job 227548477: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:55 2022
Job was executed on host(s) <2*eu-a2p-195>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sat Aug  6 21:43:41 2022
Results reported at Sat Aug  6 21:43:41 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   114160.34 sec.
    Max Memory :                                 2666 MB
    Average Memory :                             2055.73 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5526.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   113536 sec.
    Turnaround time :                            125086 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:06.084056: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-138>
Subject: Job 227548294: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:30 2022
Job was executed on host(s) <2*eu-a2p-138>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 01:23:53 2022
Results reported at Sun Aug  7 01:23:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   126595.91 sec.
    Max Memory :                                 6402 MB
    Average Memory :                             3652.85 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1790.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   126738 sec.
    Turnaround time :                            138323 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:15.740556: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-191>
Subject: Job 227548320: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:33 2022
Job was executed on host(s) <2*eu-a2p-191>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 01:31:28 2022
Results reported at Sun Aug  7 01:31:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   127072.98 sec.
    Max Memory :                                 6570 MB
    Average Memory :                             3805.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1622.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                41
    Run time :                                   127204 sec.
    Turnaround time :                            138775 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:07.806356: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-242>
Subject: Job 227548322: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:33 2022
Job was executed on host(s) <2*eu-a2p-242>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 02:25:56 2022
Results reported at Sun Aug  7 02:25:56 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   130297.30 sec.
    Max Memory :                                 6559 MB
    Average Memory :                             3798.15 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1633.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   130471 sec.
    Turnaround time :                            142043 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:24.033902: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-249>
Subject: Job 227548305: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:31 2022
Job was executed on host(s) <2*eu-a2p-249>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 02:30:53 2022
Results reported at Sun Aug  7 02:30:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   130654.81 sec.
    Max Memory :                                 6452 MB
    Average Memory :                             3689.40 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1740.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   130756 sec.
    Turnaround time :                            142342 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:27.451579: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-169>
Subject: Job 227548349: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:36 2022
Job was executed on host(s) <2*eu-a2p-169>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 02:49:37 2022
Results reported at Sun Aug  7 02:49:37 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   131831.45 sec.
    Max Memory :                                 6899 MB
    Average Memory :                             3881.03 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1293.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   131894 sec.
    Turnaround time :                            143461 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:35.160957: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-191>
Subject: Job 227548308: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:32 2022
Job was executed on host(s) <2*eu-a2p-191>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 02:49:58 2022
Results reported at Sun Aug  7 02:49:58 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   131744.31 sec.
    Max Memory :                                 6570 MB
    Average Memory :                             3806.09 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1622.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   131913 sec.
    Turnaround time :                            143486 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:23.972597: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-242>
Subject: Job 227548335: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:35 2022
Job was executed on host(s) <2*eu-a2p-242>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 02:50:05 2022
Results reported at Sun Aug  7 02:50:05 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   131797.73 sec.
    Max Memory :                                 6696 MB
    Average Memory :                             3641.33 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1496.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   131919 sec.
    Turnaround time :                            143490 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:20.513896: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-138>
Subject: Job 227548288: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:29 2022
Job was executed on host(s) <2*eu-a2p-138>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 03:11:31 2022
Results reported at Sun Aug  7 03:11:31 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   133068.64 sec.
    Max Memory :                                 6421 MB
    Average Memory :                             3659.90 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1771.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   133197 sec.
    Turnaround time :                            144782 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:25.537397: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-191>
Subject: Job 227548313: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:32 2022
Job was executed on host(s) <2*eu-a2p-191>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 03:11:36 2022
Results reported at Sun Aug  7 03:11:36 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   133089.75 sec.
    Max Memory :                                 6439 MB
    Average Memory :                             3677.61 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1753.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   133212 sec.
    Turnaround time :                            144784 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:24.557405: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-249>
Subject: Job 227548296: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:30 2022
Job was executed on host(s) <2*eu-a2p-249>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 03:27:20 2022
Results reported at Sun Aug  7 03:27:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   133977.39 sec.
    Max Memory :                                 6550 MB
    Average Memory :                             3789.02 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1642.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   134143 sec.
    Turnaround time :                            145730 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:13.501080: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-192>
Subject: Job 227548362: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:38 2022
Job was executed on host(s) <2*eu-a2p-192>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 03:43:30 2022
Results reported at Sun Aug  7 03:43:30 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   134989.58 sec.
    Max Memory :                                 6788 MB
    Average Memory :                             3737.07 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1404.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   135126 sec.
    Turnaround time :                            146692 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:36.453404: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-249>
Subject: Job 227548299: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:31 2022
Job was executed on host(s) <2*eu-a2p-249>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 03:53:10 2022
Results reported at Sun Aug  7 03:53:10 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   135643.75 sec.
    Max Memory :                                 6585 MB
    Average Memory :                             3821.92 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1607.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   135692 sec.
    Turnaround time :                            147279 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:04.871096: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-169>
Subject: Job 227548337: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:35 2022
Job was executed on host(s) <2*eu-a2p-169>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 04:07:19 2022
Results reported at Sun Aug  7 04:07:19 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   136407.33 sec.
    Max Memory :                                 6825 MB
    Average Memory :                             3773.83 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1367.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   136555 sec.
    Turnaround time :                            148124 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:34.470960: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-192>
Subject: Job 227548351: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:37 2022
Job was executed on host(s) <2*eu-a2p-192>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 04:22:25 2022
Results reported at Sun Aug  7 04:22:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   137289.33 sec.
    Max Memory :                                 6863 MB
    Average Memory :                             3810.56 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1329.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   137460 sec.
    Turnaround time :                            149028 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:19.323404: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-242>
Subject: Job 227548328: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:34 2022
Job was executed on host(s) <2*eu-a2p-242>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 04:36:04 2022
Results reported at Sun Aug  7 04:36:04 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   138159.41 sec.
    Max Memory :                                 6865 MB
    Average Memory :                             3813.54 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1327.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   138278 sec.
    Turnaround time :                            149850 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:15.533909: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-268>
Subject: Job 227548363: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:38 2022
Job was executed on host(s) <2*eu-a2p-268>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 04:40:13 2022
Results reported at Sun Aug  7 04:40:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   138304.05 sec.
    Max Memory :                                 6835 MB
    Average Memory :                             3782.21 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1357.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   138498 sec.
    Turnaround time :                            150095 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:19.854561: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-216>
Subject: Job 227548502: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:00 2022
Job was executed on host(s) <2*eu-a2p-216>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 05:04:58 2022
Results reported at Sun Aug  7 05:04:58 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   164020.36 sec.
    Max Memory :                                 2512 MB
    Average Memory :                             1886.25 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5680.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   139998 sec.
    Turnaround time :                            151558 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:25.824144: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-268>
Subject: Job 227548370: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:39 2022
Job was executed on host(s) <2*eu-a2p-268>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 05:08:00 2022
Results reported at Sun Aug  7 05:08:00 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   140060.72 sec.
    Max Memory :                                 6702 MB
    Average Memory :                             3649.56 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1490.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   140165 sec.
    Turnaround time :                            151761 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:32.214561: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-216>
Subject: Job 227548495: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:57 2022
Job was executed on host(s) <2*eu-a2p-216>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 05:09:30 2022
Results reported at Sun Aug  7 05:09:30 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   164458.16 sec.
    Max Memory :                                 2670 MB
    Average Memory :                             1946.19 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5522.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   140271 sec.
    Turnaround time :                            151833 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:08.755107: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-192>
Subject: Job 227548357: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:37 2022
Job was executed on host(s) <2*eu-a2p-192>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 05:13:03 2022
Results reported at Sun Aug  7 05:13:03 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   140366.42 sec.
    Max Memory :                                 6700 MB
    Average Memory :                             3650.67 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1492.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   140499 sec.
    Turnaround time :                            152066 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:35.593407: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-169>
Subject: Job 227548343: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:36 2022
Job was executed on host(s) <2*eu-a2p-169>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 05:15:28 2022
Results reported at Sun Aug  7 05:15:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   140573.38 sec.
    Max Memory :                                 6701 MB
    Average Memory :                             3649.41 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1491.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   140645 sec.
    Turnaround time :                            152212 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:27.800970: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-201>
Subject: Job 227548482: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:56 2022
Job was executed on host(s) <2*eu-a2p-201>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 05:29:38 2022
Results reported at Sun Aug  7 05:29:38 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   165648.75 sec.
    Max Memory :                                 2674 MB
    Average Memory :                             2066.30 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5518.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   141492 sec.
    Turnaround time :                            153042 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:12.225157: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-193>
Subject: Job 227548515: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:01 2022
Job was executed on host(s) <2*eu-a2p-193>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 05:51:18 2022
Results reported at Sun Aug  7 05:51:18 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   167290.86 sec.
    Max Memory :                                 2603 MB
    Average Memory :                             1928.35 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5589.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   142793 sec.
    Turnaround time :                            154337 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:26.290189: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-195>
Subject: Job 227548473: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:55 2022
Job was executed on host(s) <2*eu-a2p-195>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 06:14:07 2022
Results reported at Sun Aug  7 06:14:07 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   168988.94 sec.
    Max Memory :                                 2520 MB
    Average Memory :                             1919.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5672.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   144162 sec.
    Turnaround time :                            155712 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:18.157817: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.78806
********************************
Sender: LSF System <lsfadmin@eu-a2p-196>
Subject: Job 227548536: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:04 2022
Job was executed on host(s) <2*eu-a2p-196>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 06:35:53 2022
Results reported at Sun Aug  7 06:35:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   170115.86 sec.
    Max Memory :                                 2597 MB
    Average Memory :                             1921.21 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5595.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   145438 sec.
    Turnaround time :                            157009 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:25.522027: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-193>
Subject: Job 227548510: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:01 2022
Job was executed on host(s) <2*eu-a2p-193>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 06:44:06 2022
Results reported at Sun Aug  7 06:44:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   170769.27 sec.
    Max Memory :                                 2588 MB
    Average Memory :                             1913.94 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5604.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   145962 sec.
    Turnaround time :                            157505 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:28.388314: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-196>
Subject: Job 227548530: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:03 2022
Job was executed on host(s) <2*eu-a2p-196>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 07:03:23 2022
Results reported at Sun Aug  7 07:03:23 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   172105.00 sec.
    Max Memory :                                 2591 MB
    Average Memory :                             1916.45 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5601.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   147088 sec.
    Turnaround time :                            158660 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:29.992074: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-149>
Subject: Job 227548558: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:08 2022
Job was executed on host(s) <2*eu-a2p-149>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 08:09:53 2022
Results reported at Sun Aug  7 08:09:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 4 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   176735.20 sec.
    Max Memory :                                 2576 MB
    Average Memory :                             1904.98 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5616.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   151083 sec.
    Turnaround time :                            162645 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:18:24.583400: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.80810
********************************
Sender: LSF System <lsfadmin@eu-a2p-138>
Subject: Job 227548291: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:29 2022
Job was executed on host(s) <2*eu-a2p-138>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 14:22:04 2022
Results reported at Sun Aug  7 14:22:04 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   184964.44 sec.
    Max Memory :                                 6579 MB
    Average Memory :                             3813.68 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1613.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   173430 sec.
    Turnaround time :                            185015 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:14.851788: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-191>
Subject: Job 227548317: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:33 2022
Job was executed on host(s) <2*eu-a2p-191>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 15:26:26 2022
Results reported at Sun Aug  7 15:26:26 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   188927.67 sec.
    Max Memory :                                 6412 MB
    Average Memory :                             3653.18 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1780.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   177302 sec.
    Turnaround time :                            188873 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:18.445401: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-249>
Subject: Job 227548298: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:30 2022
Job was executed on host(s) <2*eu-a2p-249>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 16:00:16 2022
Results reported at Sun Aug  7 16:00:16 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   191018.78 sec.
    Max Memory :                                 6422 MB
    Average Memory :                             3651.02 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1770.00 MB
    Max Swap :                                   300 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   179318 sec.
    Turnaround time :                            190906 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:24.169102: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-249>
Subject: Job 227548301: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:31 2022
Job was executed on host(s) <2*eu-a2p-249>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 16:05:14 2022
Results reported at Sun Aug  7 16:05:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   191270.92 sec.
    Max Memory :                                 6404 MB
    Average Memory :                             3622.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1788.00 MB
    Max Swap :                                   576 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   179616 sec.
    Turnaround time :                            191203 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:20.654107: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-242>
Subject: Job 227548325: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:34 2022
Job was executed on host(s) <2*eu-a2p-242>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 16:13:24 2022
Results reported at Sun Aug  7 16:13:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   191789.39 sec.
    Max Memory :                                 6414 MB
    Average Memory :                             3654.91 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1778.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   180118 sec.
    Turnaround time :                            191690 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:25.028925: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-169>
Subject: Job 227548345: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:36 2022
Job was executed on host(s) <2*eu-a2p-169>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 16:34:22 2022
Results reported at Sun Aug  7 16:34:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   193167.31 sec.
    Max Memory :                                 6676 MB
    Average Memory :                             3627.91 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1516.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   181378 sec.
    Turnaround time :                            192946 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:20.889360: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-242>
Subject: Job 227548331: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:34 2022
Job was executed on host(s) <2*eu-a2p-242>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 17:19:18 2022
Results reported at Sun Aug  7 17:19:18 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   195816.92 sec.
    Max Memory :                                 6704 MB
    Average Memory :                             3654.65 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1488.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   184072 sec.
    Turnaround time :                            195644 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:28.103958: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-192>
Subject: Job 227548359: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:38 2022
Job was executed on host(s) <2*eu-a2p-192>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 17:22:02 2022
Results reported at Sun Aug  7 17:22:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   195999.48 sec.
    Max Memory :                                 6745 MB
    Average Memory :                             3718.04 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1447.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   184238 sec.
    Turnaround time :                            195804 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:36.651457: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-192>
Subject: Job 227548354: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:37 2022
Job was executed on host(s) <2*eu-a2p-192>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 18:00:03 2022
Results reported at Sun Aug  7 18:00:03 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   198367.64 sec.
    Max Memory :                                 6703 MB
    Average Memory :                             3652.44 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1489.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   186519 sec.
    Turnaround time :                            198086 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:31.604696: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-268>
Subject: Job 227548366: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:58:39 2022
Job was executed on host(s) <2*eu-a2p-268>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Sun Aug  7 18:15:18 2022
Results reported at Sun Aug  7 18:15:18 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 4 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   199302.55 sec.
    Max Memory :                                 6863 MB
    Average Memory :                             3813.53 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1329.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   187403 sec.
    Turnaround time :                            198999 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:16.819585: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548600: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:13 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:16 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:16 2022
Terminated at Sun Aug  7 23:55:44 2022
Results reported at Sun Aug  7 23:55:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   208589.25 sec.
    Max Memory :                                 6098 MB
    Average Memory :                             3750.08 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2094.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   207688 sec.
    Turnaround time :                            219391 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:28.746923: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-149>
Subject: Job 227548567: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:09 2022
Job was executed on host(s) <2*eu-a2p-149>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Mon Aug  8 04:42:54 2022
Results reported at Mon Aug  8 04:42:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   225455.19 sec.
    Max Memory :                                 6236 MB
    Average Memory :                             3786.48 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1956.00 MB
    Max Swap :                                   497 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   225065 sec.
    Turnaround time :                            236625 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:05.483247: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-172>
Subject: Job 227548648: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:23 2022
Job was executed on host(s) <2*eu-a2p-172>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:19 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:19 2022
Terminated at Mon Aug  8 05:00:04 2022
Results reported at Mon Aug  8 05:00:04 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   226549.56 sec.
    Max Memory :                                 6298 MB
    Average Memory :                             3515.59 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1894.00 MB
    Max Swap :                                   1115 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   225946 sec.
    Turnaround time :                            237641 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:28.377717: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-202>
Subject: Job 227548582: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:11 2022
Job was executed on host(s) <2*eu-a2p-202>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Mon Aug  8 05:03:33 2022
Results reported at Mon Aug  8 05:03:33 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   226618.70 sec.
    Max Memory :                                 6347 MB
    Average Memory :                             3698.07 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1845.00 MB
    Max Swap :                                   868 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   226327 sec.
    Turnaround time :                            237862 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:15.011474: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548594: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:12 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:14 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:14 2022
Terminated at Mon Aug  8 05:16:03 2022
Results reported at Mon Aug  8 05:16:03 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   227706.69 sec.
    Max Memory :                                 6039 MB
    Average Memory :                             3444.99 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2153.00 MB
    Max Swap :                                   1670 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   226907 sec.
    Turnaround time :                            238611 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:23.111954: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-149>
Subject: Job 227548602: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:13 2022
Job was executed on host(s) <2*eu-a2p-149>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:15 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:15 2022
Terminated at Mon Aug  8 06:10:10 2022
Results reported at Mon Aug  8 06:10:10 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   230409.38 sec.
    Max Memory :                                 6065 MB
    Average Memory :                             3641.26 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2127.00 MB
    Max Swap :                                   467 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   230155 sec.
    Turnaround time :                            241857 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:25.983249: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-153>
Subject: Job 227548591: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:12 2022
Job was executed on host(s) <2*eu-a2p-153>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:13 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:13 2022
Terminated at Mon Aug  8 06:29:44 2022
Results reported at Mon Aug  8 06:29:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   232131.62 sec.
    Max Memory :                                 6157 MB
    Average Memory :                             3813.68 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2035.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                41
    Run time :                                   231330 sec.
    Turnaround time :                            243032 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:25.795650: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-211>
Subject: Job 227548630: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:19 2022
Job was executed on host(s) <2*eu-a2p-211>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:15:06 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:15:06 2022
Terminated at Mon Aug  8 07:05:14 2022
Results reported at Mon Aug  8 07:05:14 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   233954.41 sec.
    Max Memory :                                 6783 MB
    Average Memory :                             4150.80 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1409.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   233417 sec.
    Turnaround time :                            245155 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:43.749382: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-202>
Subject: Job 227548571: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:09 2022
Job was executed on host(s) <2*eu-a2p-202>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Mon Aug  8 07:48:10 2022
Results reported at Mon Aug  8 07:48:10 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   237093.00 sec.
    Max Memory :                                 6060 MB
    Average Memory :                             3680.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2132.00 MB
    Max Swap :                                   36 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   236204 sec.
    Turnaround time :                            247741 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:16.191454: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-149>
Subject: Job 227548634: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:19 2022
Job was executed on host(s) <2*eu-a2p-149>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:15 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:15 2022
Terminated at Mon Aug  8 08:11:33 2022
Results reported at Mon Aug  8 08:11:33 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   237903.03 sec.
    Max Memory :                                 6325 MB
    Average Memory :                             3642.48 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1867.00 MB
    Max Swap :                                   264 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   237438 sec.
    Turnaround time :                            249134 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:29.533248: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-202>
Subject: Job 227548577: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:10 2022
Job was executed on host(s) <2*eu-a2p-202>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Mon Aug  8 08:27:32 2022
Results reported at Mon Aug  8 08:27:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   239743.88 sec.
    Max Memory :                                 6042 MB
    Average Memory :                             3661.94 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2150.00 MB
    Max Swap :                                   61 MB
    Max Processes :                              5
    Max Threads :                                41
    Run time :                                   238567 sec.
    Turnaround time :                            250102 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:28.801470: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-172>
Subject: Job 227548639: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:20 2022
Job was executed on host(s) <2*eu-a2p-172>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:16 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:16 2022
Terminated at Mon Aug  8 09:53:11 2022
Results reported at Mon Aug  8 09:53:11 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   288959.88 sec.
    Max Memory :                                 6345 MB
    Average Memory :                             3517.13 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1847.00 MB
    Max Swap :                                   1172 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   243533 sec.
    Turnaround time :                            255231 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:27.652332: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548597: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:13 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:16 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:16 2022
Terminated at Mon Aug  8 10:42:27 2022
Results reported at Mon Aug  8 10:42:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   248435.05 sec.
    Max Memory :                                 6056 MB
    Average Memory :                             3710.65 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2136.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   246491 sec.
    Turnaround time :                            258194 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:32.296955: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-149>
Subject: Job 227548565: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:09 2022
Job was executed on host(s) <2*eu-a2p-149>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Tue Aug  9 01:42:20 2022
Results reported at Tue Aug  9 01:42:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   351053.84 sec.
    Max Memory :                                 6225 MB
    Average Memory :                             3635.43 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1967.00 MB
    Max Swap :                                   492 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   300631 sec.
    Turnaround time :                            312191 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:25.628973: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-191>
Subject: Job 227548588: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:12 2022
Job was executed on host(s) <2*eu-a2p-191>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:15 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:15 2022
Terminated at Tue Aug  9 02:14:05 2022
Results reported at Tue Aug  9 02:14:05 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   353966.31 sec.
    Max Memory :                                 6069 MB
    Average Memory :                             3748.09 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2123.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   302389 sec.
    Turnaround time :                            314093 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:20.371671: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-201>
Subject: Job 227548587: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:11 2022
Job was executed on host(s) <2*eu-a2p-201>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:15 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:15 2022
Terminated at Tue Aug  9 02:18:17 2022
Results reported at Tue Aug  9 02:18:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   353328.22 sec.
    Max Memory :                                 6057 MB
    Average Memory :                             3734.75 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2135.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   302642 sec.
    Turnaround time :                            314346 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:17.929078: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-202>
Subject: Job 227548580: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:10 2022
Job was executed on host(s) <2*eu-a2p-202>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Tue Aug  9 03:14:29 2022
Results reported at Tue Aug  9 03:14:29 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   357561.56 sec.
    Max Memory :                                 6060 MB
    Average Memory :                             3472.52 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2132.00 MB
    Max Swap :                                   497 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   306184 sec.
    Turnaround time :                            317719 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:26.179100: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-169>
Subject: Job 227548585: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:11 2022
Job was executed on host(s) <2*eu-a2p-169>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:14 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:14 2022
Terminated at Tue Aug  9 03:30:40 2022
Results reported at Tue Aug  9 03:30:40 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   358657.75 sec.
    Max Memory :                                 6069 MB
    Average Memory :                             3746.32 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2123.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                41
    Run time :                                   306986 sec.
    Turnaround time :                            318689 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:30.044222: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-211>
Subject: Job 227548642: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:20 2022
Job was executed on host(s) <2*eu-a2p-211>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:15:06 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:15:06 2022
Terminated at Tue Aug  9 03:45:37 2022
Results reported at Tue Aug  9 03:45:37 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   360027.19 sec.
    Max Memory :                                 6438 MB
    Average Memory :                             3816.90 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1754.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   307841 sec.
    Turnaround time :                            319577 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:37.463805: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-138>
Subject: Job 227548611: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:16 2022
Job was executed on host(s) <2*eu-a2p-138>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:19 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:19 2022
Terminated at Tue Aug  9 03:49:24 2022
Results reported at Tue Aug  9 03:49:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   359786.31 sec.
    Max Memory :                                 6416 MB
    Average Memory :                             2900.86 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1776.00 MB
    Max Swap :                                   2535 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   308106 sec.
    Turnaround time :                            319808 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:29.390451: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548605: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:16 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:15 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:15 2022
Terminated at Tue Aug  9 04:03:07 2022
Results reported at Tue Aug  9 04:03:07 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   360597.22 sec.
    Max Memory :                                 6458 MB
    Average Memory :                             3305.72 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1734.00 MB
    Max Swap :                                   1426 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   308932 sec.
    Turnaround time :                            320631 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:24.550840: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-202>
Subject: Job 227548574: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:10 2022
Job was executed on host(s) <2*eu-a2p-202>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:11:51 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:11:51 2022
Terminated at Tue Aug  9 04:14:53 2022
Results reported at Tue Aug  9 04:14:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   361455.28 sec.
    Max Memory :                                 6158 MB
    Average Memory :                             2871.74 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               2034.00 MB
    Max Swap :                                   1989 MB
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   309808 sec.
    Turnaround time :                            321343 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:24.692166: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.150354
********************************
Sender: LSF System <lsfadmin@eu-a2p-190>
Subject: Job 227548628: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:19 2022
Job was executed on host(s) <2*eu-a2p-190>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:15:06 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:15:06 2022
Terminated at Tue Aug  9 04:21:59 2022
Results reported at Tue Aug  9 04:21:59 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   361858.75 sec.
    Max Memory :                                 6290 MB
    Average Memory :                             3670.93 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1902.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                41
    Run time :                                   310040 sec.
    Turnaround time :                            321760 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:27.356850: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-153>
Subject: Job 227548617: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:17 2022
Job was executed on host(s) <2*eu-a2p-153>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:20 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:20 2022
Terminated at Tue Aug  9 04:57:47 2022
Results reported at Tue Aug  9 04:57:47 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.995
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   364115.41 sec.
    Max Memory :                                 6325 MB
    Average Memory :                             3711.63 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1867.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                41
    Run time :                                   312207 sec.
    Turnaround time :                            323910 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:31.330798: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-167>
Subject: Job 227548608: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:16 2022
Job was executed on host(s) <2*eu-a2p-167>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:15 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:15 2022
Terminated at Tue Aug  9 05:07:06 2022
Results reported at Tue Aug  9 05:07:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   364317.44 sec.
    Max Memory :                                 6325 MB
    Average Memory :                             2949.62 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1867.00 MB
    Max Swap :                                   2138 MB
    Max Processes :                              5
    Max Threads :                                41
    Run time :                                   312770 sec.
    Turnaround time :                            324470 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:36.843594: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-239>
Subject: Job 227548651: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:23 2022
Job was executed on host(s) <2*eu-a2p-239>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:20 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:20 2022
Terminated at Tue Aug  9 06:03:49 2022
Results reported at Tue Aug  9 06:03:49 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   369195.09 sec.
    Max Memory :                                 6292 MB
    Average Memory :                             3667.65 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1900.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   316169 sec.
    Turnaround time :                            327866 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:24.555530: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-239>
Subject: Job 227548614: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:17 2022
Job was executed on host(s) <2*eu-a2p-239>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:19 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:19 2022
Terminated at Tue Aug  9 06:28:23 2022
Results reported at Tue Aug  9 06:28:23 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs False --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   370288.53 sec.
    Max Memory :                                 6425 MB
    Average Memory :                             3810.78 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1767.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   317643 sec.
    Turnaround time :                            329346 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:37.554814: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-180>
Subject: Job 227548620: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:17 2022
Job was executed on host(s) <2*eu-a2p-180>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:15 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:15 2022
Terminated at Tue Aug  9 06:34:05 2022
Results reported at Tue Aug  9 06:34:05 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.001 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   369444.34 sec.
    Max Memory :                                 6338 MB
    Average Memory :                             3725.71 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1854.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   317990 sec.
    Turnaround time :                            329688 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:34.371038: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a2p-268>
Subject: Job 227548637: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:20 2022
Job was executed on host(s) <2*eu-a2p-268>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Fri Aug  5 14:14:16 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Fri Aug  5 14:14:16 2022
Terminated at Tue Aug  9 06:48:50 2022
Results reported at Tue Aug  9 06:48:50 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.2 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   371964.72 sec.
    Max Memory :                                 6361 MB
    Average Memory :                             3744.53 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1831.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                41
    Run time :                                   318873 sec.
    Turnaround time :                            330570 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-05 14:19:30.689990: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
Sender: LSF System <lsfadmin@eu-a6-001-23>
Subject: Job 227548646: <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> in cluster <euler> Done

Job <python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9> was submitted from host <eu-c7-119-05> by user <garcieri> in cluster <euler> at Fri Aug  5 10:59:21 2022
Job was executed on host(s) <2*eu-a6-001-23>, in queue <normal.120h>, as user <garcieri> in cluster <euler> at Sat Aug  6 05:01:42 2022
</cluster/home/garcieri> was used as the home directory.
</cluster/home/garcieri/rlfr/Robust-optimal-maintenance-planning-through-reinforcement-learning-and-domain-randomization> was used as the working directory.
Started at Sat Aug  6 05:01:42 2022
Terminated at Fri Aug 12 13:02:51 2022
Results reported at Fri Aug 12 13:02:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python -m transformer.run_gridsearch --seed 0 --train_episodes 10000 --test_episodes 500 --update_iterations 10 --gradient_descent_epochs 1 --num_heads 8 --num_layers 8 --hidden_sizes_mlp [100] --learning_rate 0.0005 --alpha 0.1 --save_rewards False --save_model False --gridsearch True --keep_last_window_lenght_obs True --polyak 0.9
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   590311.44 sec.
    Max Memory :                                 6470 MB
    Average Memory :                             3845.52 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               1722.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                47
    Run time :                                   547274 sec.
    Turnaround time :                            612210 sec.

The output (if any) follows:

/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead
  warnings.warn('jax.experimental.optimizers is deprecated, '
/cluster/home/garcieri/miniconda3/envs/rlfr/lib/python3.8/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead
  warnings.warn('jax.experimental.stax is deprecated, '
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
2022-08-06 05:06:03.490533: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] 
********************************
Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Compiling module jit_update.154362
********************************
